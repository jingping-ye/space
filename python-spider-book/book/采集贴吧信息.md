# 采集贴吧信息

打开贴吧的某一个帖子，将源代码保存下来为source.txt,提取其中的用户名、发帖内容和发帖时间，保存为result.csv页面。

## 分析页面

### 用户名

```html
<a 
 data-field="{&quot;un&quot;:&quot;\u4f60\u978b\u5e95\u7834\u4e86&quot;,&quot;id&quot;:&quot;255fe4bda0e99e8be5ba95e7a0b4e4ba86003a?t=1387688649&quot;}" 
alog-group="p_author" 
class="p_author_name j_user_card" 
href="/home/main?un=%E4%BD%A0%E9%9E%8B%E5%BA%95%E7%A0%B4%E4%BA%86&amp;ie=utf-8&amp;id=255fe4bda0e99e8be5ba95e7a0b4e4ba86003a?t=1387688649&amp;fr=pb" 
target="_blank">你鞋底破了</a>
```

有个专门的`class="p_author_name j_user_card"`

另一个地方

```html
<img username="你鞋底破了" class="" src="https://gss0.bdstatic.com/6LZ1dD3d1sgCo2Kml5_Y_D3/sys/portrait/item/255fe4bda0e99e8be5ba95e7a0b4e4ba86003a?t=1387688649"/></a>
```

#### 正则

```python
username="(.*?)"
```

### 帖子内容

```html
<div 
     id="post_content_127415910631" 
     class="d_post_content j_d_post_content " 
     style="display:;">            
    其实网络暴力才是目前最大的阻碍，其他方面一时半会也解决不了，男篮本来实力就不行，靠着东道主主场优势跟一口气吊着的，这可好，打波兰输了看看那网络暴力，精气神儿都给喷没了，运动员估计没几个睡的着的，打委内瑞拉就显露出来了，想使劲使不出，拼都不知道该咋拼了，真棒，喷子们继续吧，讨论归讨论，施压归施压，网络暴力是几个意思？看看那些新出的名次，你们是爽了，男篮成绩会更差的</div>
```

有个专门的`class="d_post_content j_d_post_content "`

#### 正则

```python
class="d_post_content j_d_post_content "(.*?)<
```

### 发帖时间

```html
<div class="post-tail-wrap">
    <span class="j_jb_ele">
        <a rel="noopener" href="###" class="tail-info" data-checkun="un">
            <img class="icon-jubao" src="//tb2.bdstatic.com/tb/static-pb/img/jubao_button_5f60185.png">
        </a>
   </span>
    <span class="tail-info">
        来自<a rel="noopener" data-tip="超萌态动画表情来袭，速度抢先体验！" href="http://c.tieba.baidu.com/c/s/download/pc?src=webtbGF" target="_blank">
        Android客户端</a>
    </span>
    <span class="tail-info">2楼</span>
    <span class="tail-info">2019-09-05 11:03</span>
</div>
```

有个`class="post-tail-wrap"`且为最后一个。x



#### 正则

```python
tail-info">(201[0-9].*?)<
```

## 方法

### 方法一 

分别匹配用户名、帖子内容、回复时间，然后组装成字典，组成csv数据

```python
import re
import csv

with open('source.txt', encoding='utf-8') as f:
    source = f.read()

result_list = []
username_list = re.findall('username="(.*?)"', source, re.S)
content_list = re.findall('d_post_content j_d_post_content " style=\"display:;\">            (.*?)<', source, re.S)
reply_time_list = re.findall('tail-info">(201[0-9].*?)<', source, re.S)

for i in range(len(username_list)):
    result = {'用户名': username_list[i], '帖子': content_list[i], '回复时间': reply_time_list[i]}
    result_list.append(result)

with open('tieba.csv', 'w', encoding='utf-8', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=['用户名', '帖子', '回复时间'])
    writer.writeheader()
    writer.writerows(result_list)
```

### 方法二

采集某一个帖子的所有数据，从数据中取出用户名、帖子内容、密码然后组装成csv数据